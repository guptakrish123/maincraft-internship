# ğŸ  House Price Prediction using Machine Learning

This repository contains my internship projects completed as part of the  
**Artificial Intelligence & Machine Learning Internship at Maincrafts Technology**.

The project focuses on building, improving, and comparing Machine Learning models
to predict house prices using the **California Housing Dataset**.

---

## ğŸ“Œ Internship Tasks Overview

### ğŸ”¹ Task 1: Linear Regression â€“ House Price Prediction
- Built a baseline Linear Regression model
- Learned the complete ML workflow from data loading to evaluation

### ğŸ”¹ Task 2: Feature Engineering & Model Comparison
- Applied feature scaling
- Trained multiple regression models
- Compared performance and selected the best model

---

## ğŸ“Š Dataset

- **California Housing Dataset**
- Source: Built-in dataset from `scikit-learn`

**Target Variable:**  
- `MedHouseVal` / `HousePrice` (Median House Value)

**Input Features Include:**  
- Median Income  
- House Age  
- Average Rooms  
- Population & Location-based features  

---

## ğŸ› ï¸ Technologies Used

- Python
- Pandas, NumPy
- Matplotlib, Seaborn
- Scikit-learn
- Jupyter Notebook / VS Code

---

## âš™ï¸ Task 1: Linear Regression (Baseline Model)

### ğŸ”¸ Objective
- Understand the complete Machine Learning workflow
- Build and evaluate a Linear Regression model

### ğŸ”¸ Steps Performed
1. Imported required libraries
2. Loaded California Housing dataset
3. Performed Exploratory Data Analysis (EDA)
4. Split data into training and testing sets
5. Trained Linear Regression model
6. Evaluated model using:
   - MAE (Mean Absolute Error)
   - RMSE (Root Mean Squared Error)
   - RÂ² Score
7. Visualized Actual vs Predicted house prices

### ğŸ”¸ Result
- Linear Regression provided a reasonable baseline for house price prediction.
- Evaluation metrics and plots are included in the notebook.

---

## âš™ï¸ Task 2: Feature Engineering & Model Comparison

### ğŸ”¸ Objective
- Improve model performance using preprocessing techniques
- Compare multiple Machine Learning models

### ğŸ”¸ Steps Performed
1. Feature scaling using `StandardScaler`
2. Trainâ€“test split on scaled data
3. Trained multiple models:
   - Linear Regression
   - Ridge Regression
   - Decision Tree Regressor
4. Evaluated models using RMSE and RÂ² score
5. Created a performance comparison table
6. Selected the best-performing model
7. Visualized Actual vs Predicted values
8. Saved the best model using `joblib`

### ğŸ”¸ Result
- Model comparison helped identify the best-performing algorithm.
- Feature scaling improved training stability and performance.

---

## ğŸ“ˆ Model Evaluation Metrics

- **MAE** â€“ Mean Absolute Error  
- **RMSE** â€“ Root Mean Squared Error  
- **RÂ² Score** â€“ Coefficient of Determination  

Lower RMSE and higher RÂ² indicate better model performance.

---

## ğŸ“‚ Files in Repository

- `task1_ml_linear_regression.ipynb`  
  â†’ Task-1: Linear Regression baseline model  

- `AI_ML_Task2_Model_Comparison.ipynb`  
  â†’ Task-2: Feature scaling, model comparison, and optimization  

- `house_price_model.pkl` / `best_house_price_model.joblib`  
  â†’ Saved trained model (optional)

- `README.md`  
  â†’ Project documentation

---

## ğŸš€ Future Improvements

- Apply advanced models like Random Forest or XGBoost
- Hyperparameter tuning
- Cross-validation
- Deploy the model using Flask or Streamlit
- Build a simple web-based prediction interface

---

## ğŸ‘¨â€ğŸ’» Author

**Krish Gupta**  
Intern â€“ Artificial Intelligence & Machine Learning  
Maincrafts Technology
